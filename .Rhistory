linear_reg(penalty = best$penalty, mixture = best$mixture) |>
set_engine(engine = "glmnet")
happy
last_re_mod <-
reg_reg_wf |>
update_model(last_reg_mod) |>
last_fit(happy_split)
last_re_mod <-
reg_reg_wf |>
update_model(last_reg_mod) |>
last_fit(happy_splits)
last_re_mod
last_re_mod |> collect_metrics()
vip::vip(last_re_mod)
last_re_mod |> extract_fit_parsnip()
last_re_mod |> extract_fit_parsnip() |>
vip::vip()
last_re_mod |> extract_fit_parsnip() |>
vip::vip(geom='dot')
last_re_mod |> extract_fit_parsnip() |>
vip::vip(geom='violin')
last_re_mod |> extract_fit_parsnip() |>
vip::vip(geom='point')
last_re_mod |>
extract_fit_parsnip() |>
vip::vip(geom='point') |>
theme_minimal()
last_re_mod |>
extract_fit_parsnip() |>
vip::vip(geom='point') +
theme_minimal()
# Specify random forest spec:
happy_rf_spec <- rand_forest() |>
set_engine("ranger") |>
set_mode("regression")
happy_rf_rs <- fit_resamples(
happy_rf_spec,
happiness_score~.,
happy_folds,
control = control_resamples()
)
cv_rf_res <- happy_rf_rs |>
collect_metrics()
cv_rf_res
# Specify random forest spec:
happy_rf_spec <- rand_forest(mtry=6) |>
set_engine("ranger") |>
set_mode("regression")
happy_rf_rs <- fit_resamples(
happy_rf_spec,
happiness_score~.,
happy_folds,
control = control_resamples()
)
happy_rf_rs <- fit_resamples(
happy_rf_spec,
happiness_score~.,
happy_folds,
control = control_resamples()
)
cv_rf_res <- happy_rf_rs |>
collect_metrics()
cv_rf_res
cv_regLM_res
grid_search <- expand_grid(
mtry= c(3,5,ncol(happy_train)-1),
min_n= c(1,5,10)
)
grid_search
happy_rf_spec <- rand_forest(mtry = tune(), min_n=tune()) |>
set_mode("regression") |>
set_engine("ranger")
rf_res <- tune_grid(
happy_rf_spec,
happiness_score~.,
resamples = happy_folds,
grid=grid_search
)
rf_res <- tune_grid(
happy_rf_spec,
happiness_score~.,
resamples = happy_folds,
grid=grid_search
)
rf_res
autoplot(rf_re, metric= "rsq")
autoplot(rf_res, metric= "rsq")
autoplot(rf_res, metric= "rmse")
best <- show_best(rf_res, metric = "rmse")
bestN
best
best <- show_best(rf_res, metric = "rmse", maximise =FALSE)
best
best <- show_best(rf_res, metric = "rmse", maximise =FALSE,n=1)
best
best <- show_best(rf_res, metric = "rmse", maximise =FALSE)
best
best <- show_best(rf_res, metric = "rmse", maximise =FALSE,n=2)
best
best <- show_best(rf_res, metric = "rmse", maximise =FALSE,n=1)
best
autoplot(rf_res, metric= "rmse")
pacman::p_load(tidymodels,tidyverse)
load("~/Documents/R/Data_Wrangling_and_Tidbits/PDS/world_happiness.RData")
set.seed(123)
# Step 1: Specify the model:
happy_base_spec <- linear_reg() |>
set_engine(engine = 'lm')
# Step 2: Set up folds(default=10)
happy_folds <-  vfold_cv(happy)
happy_base_results <-  fit_resamples(
happy_base_spec,
happiness_score ~ democratic_quality + generosity + log_gdp_per_capita,
happy_folds,
control = control_resamples(save_pred = TRUE)
)
cv_res <-  happy_base_results %>%
collect_metrics()
# Results are average across the iterations
set.seed(123)
# Step 1: Specify the model:
happy_base_spec <- linear_reg() |>
set_engine(engine = 'lm')
# Step 2: Set up folds(default=10)
happy_folds <-  vfold_cv(happy)
happy_base_results <-  fit_resamples(
happy_base_spec,
happiness_score ~ democratic_quality + generosity + log_gdp_per_capita,
happy_folds,
control = control_resamples(save_pred = TRUE)
)
happy_prepped <- happy |>
select(-country, -gini_index_world_bank_estimate, -dystopia_residual) |>
recipe(happiness_score~.) |>
step_scale(everything()) |>
step_naomit(happiness_score) |>
prep() |>
bake(happy)
happy_folds <- happy_prepped |>
drop_na() |>
vfold_cv()
happy_regLM_Speci <- linear_reg(penalty = 0.001, mixture = .5) |>
set_engine(engine = "glmnet")
happy_regLM_results <- fit_resamples(
happy_regLM_Speci,
happiness_score~.,
happy_folds,
control = control_resamples(save_pred = TRUE)
)
cv_regLM_res <- happy_regLM_results |>
collect_metrics()
cv_regLM_res
# Create splits:
happy_splits <- happy |>
select(-country,-gini_index_world_bank_estimate,-dystopia_residual) |>
initial_split(prop = 0.75)
happy_train <- training(happy_splits)
happy_test <- testing(happy_splits)
happy_recipe <- recipe(happiness_score~., data =happy_train) |>
step_impute_knn(everything()) |>
step_center(everything())
happy_prepped <- happy_train |>
recipe(happiness_score ~. ) |>
step_impute_knn(everything()) |>
step_center(everything()) |>
prep()
happy_test_normalised <- bake(happy_prepped, new_data = happy_test, everything())
happy_fold <- happy_prepped |>
bake(happy) |>
vfold_cv()
# Specify which parameters to tune:
happy_regLm_spec <- linear_reg(penalty = tune(),mixture = tune()) |>
set_engine(engine = "glmnet")
# Create tuning grid:
grid_search <- expand_grid(
penalty=exp(seq(-4,-.25,length.out=10)),
mixture = seq(0,1,length.out=10)
)
reg_reg_wf <- workflow(preprocessor = happy_recipe, spec = happy_regLm_spec)
# Tune the model on the grid:
regLM_tune <- tune_grid(
reg_reg_wf,
resamples = happy_folds,
grid = grid_search
)
autoplot(regLM_tune, metric = "rmse")+geom_smooth(se=FALSE)
regLM_tune |> select_best(metric ='rmse') ->best
last_reg_mod <-
linear_reg(penalty = best$penalty, mixture = best$mixture) |>
set_engine(engine = "glmnet")
last_re_mod <-
reg_reg_wf |>
update_model(last_reg_mod) |>
last_fit(happy_splits)
last_re_mod |> collect_metrics()
last_re_mod |>
extract_fit_parsnip() |>
vip::vip(geom='point') +
theme_minimal()
# Specify random forest spec:
happy_rf_spec <- rand_forest(mtry=6) |>
set_engine("ranger") |>
set_mode("regression")
happy_rf_rs <- fit_resamples(
happy_rf_spec,
happiness_score~.,
happy_folds,
control = control_resamples()
)
cv_rf_res <- happy_rf_rs |>
collect_metrics()
cv_rf_res
grid_search <- expand_grid(
mtry= c(3,5,ncol(happy_train)-1),
min_n= c(1,5,10)
)
happy_rf_spec <- rand_forest(mtry = tune(), min_n=tune()) |>
set_mode("regression") |>
set_engine("ranger")
rf_res <- tune_grid(
happy_rf_spec,
happiness_score~.,
resamples = happy_folds,
grid=grid_search
)
autoplot(rf_res, metric= "rmse")
best <- show_best(rf_res, metric = "rmse", maximise =FALSE,n=1)
best
rf_wf <- workflow(preprocessor = happy_recipe, spec = happy_rf_spec)
last_rf_mod <-
randomforest(mtry=5,min_n=1) |>
set_engine("ranger", importance="impurity") |>
set_mode("regression")
last_rf_mod <-
rand_forest(mtry=5,min_n=1) |>
set_engine("ranger", importance="impurity") |>
set_mode("regression")
last_rf_workflow <-
rf_wf |>
update_model(last_rf_mod)
last_rf_fit <-
last_rf_workflow |>
last_fit(happy_splits)
last_rf_fit
last_rf_fit |> unnest()
last_rf_fit
last_rf_fit |>
collect_metrics()
last_rf_fit |> vip::vip()
last_rf_fit
last_rf_fit |>
pluck(".workflow",1) |>
extract_fit_parsnip() |>
vip::vip(geom="col")
load("~/Documents/R/Data_Wrangling_and_Tidbits/PDS/google_apps.RData")
pacman::p_load(tidymodels,tidyverse)
load("~/Documents/R/Data_Wrangling_and_Tidbits/PDS/google_apps.RData")
google_apps |> colnames()
# Step 1, create splits:
google_splits <- google_apps |>
select(avg_sentiment_polarity,rating, type,installs,reviews,size_in_MB,category) |>
drop_na()
load("~/Documents/R/Data_Wrangling_and_Tidbits/PDS/google_apps.RData")
# Step 1, create splits:
google_modelling <- google_apps |>
select(avg_sentiment_polarity,rating, type,installs,reviews,size_in_MB,category) |>
drop_na()
google_splits <-  google_modelling |>
initial_split(prop = 0.75,
strata = rating)
google_train <- training(google_splits)
google_test <- testing(google_splits)
# Step 2 specify recipe:
rf_recipe <- recipe(rating~.,data=google_train)
# Step 3 specify model specifications:
rf_mod <- rand_forest(mtry=2,trees=1000) |>
set_engine("ranger", importance = "impurity") |>
set_mode("regression")
# Step 4 Put workflow together:
rf_wf <-
workflow() |>
add_model(rf_mod) |>
add_recipe(rf_recipe)
rf_wf
rf_wf |>
fit(google_train)
rf_wf |>
fit(google_train) |> collect_metrics()
rf_wf |>
last_fit(google_splits)
rf_wf |>
last_fit(google_splits) |> collect_metrics()
google_folds <- google_train |>
vfold_cv()
google_rf_folds <- fit_resamples(
rf_mod,
rf_recipe,
google_folds,
control = control_resamples()
)
google_rf_rs <- fit_resamples(
rf_mod,
rating~.,
google_folds,
control = control_resamples()
)
google_folds
?fit_resamples
google_folds <- google_train |>
vfold_cv(v=5)
?fit_resamples
google_folds
control <- control_resamples(save_pred = TRUE)
google_rf_rs <- fit_resamples(
rf_mod,
rf_recipe,
google_folds,
control = control
)
fit_resamples(
rf_mod,
rf_recipe,
google_folds,
control = control
)
google_rf_rs <- fit_resamples(
rf_mod,
rf_recipe,
google_folds,
control = control
)
google_rf_rs
google_rf_rs |> collect_metrics()
rf_res
rf_wf |>
last_fit(google_splits) |> collect_metrics()->rf_res
rf_res
rf_mod <-
rand_forest(mtry=tune(),
min_n = tune(),
trees= 1000)
rf_mod <-
rand_forest(mtry=tune(),
min_n = tune(),
trees= 1000) |>
set_engine("ranger") |>
set_mode("regression", importance="impurity")
rf_mod <-
rand_forest(mtry=tune(),
min_n = tune(),
trees= 1000) |>
set_engine("ranger") |>
set_mode("regression",  importance = "impurity")
rf_mod <-
rand_forest(mtry=tune(),
min_n = tune(),
trees= 1000) |>
set_engine("ranger") |>
set_mode("regression")
# validation split:
val_set <-
validation_split(google_train,
strata= rating,
prop=0.8)
# check tunable params:
rf_mod |>
parameters()
# Results:
set.seed(1234)
# create workflow:
rf_wf <- workflow() |>
add_model(rf_mod) |>
add_recipe(rf_recipe)
rf_res <- rf_wf |>
tune_grid(val_set,
grid=25,
control= control_grid(save_pred = TRUE))
rf_res
val_set
rf_res <- rf_wf |>
tune_grid(google_folds,
grid=25,
control= control_grid(save_pred = TRUE))
rf_res <- rf_wf |>
tune_grid(val_set,
grid=25,
control= control_grid(save_pred = TRUE))
rf_res <- rf_wf |>
tune_grid(val_set,
grid=25,
control= control_grid(save_pred = TRUE))
rf_res
rf_res |>
show_best()
rf_res |>
autoplot(metric="rmse")
rf_res |>
show_best()
rf_res |>
show_best()
rf_res |>
show_best(n=1)
# update workflow with best results:
last_rf_mod <- rand_forest(mtry=2,
min_n = 16,
trees= 1000) |>
set_engine("ranger") |>
set_mode("regression")
last_rf_workflow <-
rf_wf |>
update_model(last_rf_mod) |>
last_split(google_splits)
last_rf_workflow <-
rf_wf |>
update_model(last_rf_mod)
google_rf_rs <- fit_resamples(
rf_wf
google_folds,
google_rf_rs <- fit_resamples(
rf_wf,
google_folds,
control = control
)
google_rf_rs$.notes
google_rf_rs$.notes[[1]]
last_rf_fit <-
last_rf_workflow |>
last_fit(google_splits)
last_rf_fit
last_rf_fit |>
collect_metrics()
# Step 2 specify recipe:
rf_recipe <- recipe(rating~.,data=google_train)
# Step 3 specify model specifications:
rf_mod <- rand_forest(mtry=2,trees=1000) |>
set_engine("ranger", importance = "impurity") |>
set_mode("regression")
colnames(google_apps)
google_folds <- google_train |>
vfold_cv(strata=rating)
# Step 3 specify model specifications:
rf_mod <- rand_forest(trees=500) |>
set_engine("ranger") |>
set_mode("regression")
# Step 4 Put workflow together:
rf_wf <-
workflow() |>
add_model(rf_mod) |>
add_recipe(rf_recipe)
google_folds <- google_train |>
vfold_cv(strata=rating)
google_folds
google_rf_rs <- fit_resamples(
rf_wf,
google_folds,
control = control
)
google_rf_rs
google_rf_rs |> collect_metrics()
google_rf_rs |> collect_predictions()
last_rf_fit |>
collect_predictions()
ggplot(aes(rating, .pred,color=id)) |>
geom_jitter(width = 0.5m, alpha=0.5)
ggplot(aes(rating, .pred,color=id)) |>
geom_jitter(width = 0.5, alpha=0.5)
last_rf_fit |>
collect_predictions() |>
ggplot(aes(rating, .pred,color=id)) |>
geom_jitter(width = 0.5, alpha=0.5)
last_rf_fit |>
collect_predictions() |>
ggplot(aes(rating, .pred,color=id)) +
geom_jitter(width = 0.5, alpha=0.5)
google_rf_rs |>
collect_predictions() |>
ggplot(aes(rating, .pred,color=id)) +
geom_jitter(width = 0.5, alpha=0.5)
last_rf_fit |>
collect_predictions() |>
autoplot()
last_rf_fit |>
collect_predictions()
last_rf_fit |>
collect_predictions()
last_rf_fit |>
collect_predictions()
last_rf_fit |>
collect_metrics()
rf_res |>
autoplot(metric="rmse")
rf_res |>
autoplot(metric="rsq")
load("~/Documents/R/Data_Wrangling_and_Tidbits/PDS/world_happiness.RData")
pacman::p_load(tidymodels,tidyverse)
set.seed(123)
# Step 1: Specify the model:
happy_base_spec <- linear_reg() |>
set_engine(engine = 'lm')
# Step 2: Set up folds(default=10)
happy_folds <-  vfold_cv(happy)
happy_base_results <-  fit_resamples(
happy_base_spec,
happiness_score ~ democratic_quality + generosity + log_gdp_per_capita,
happy_folds,
control = control_resamples(save_pred = TRUE)
)
cv_res <-  happy_base_results %>%
collect_metrics()
# Results are average across the iterations
cv_res
parsnip:::parsnip_addin()
